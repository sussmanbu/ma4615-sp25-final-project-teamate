[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This page details our data sources, the cleaning process, and provides initial diagnostic plots. For this project, we work with data from the landmark study Systemic Discrimination Among Large U.S. Employers (Kline, Rose, and Walters, 2022). The data was originally collected through a fake resume experiment to investigate patterns of hiring discrimination in large US employers, making it highly relevant to both current social justice issues and policy enforcement efforts."
  },
  {
    "objectID": "data.html#data-sources-and-rationale",
    "href": "data.html#data-sources-and-rationale",
    "title": "Data",
    "section": "Data Sources and Rationale",
    "text": "Data Sources and Rationale\n\nExperimental, Main Dataset\n\nSource: The dataset is available on Harvard Dataverse and was published alongside the study “Systemic Discrimination Among Large U.S. Employers”.\nLink to original data source\nPurpose: The data were collected to examine whether hiring discrimination is endemic to particular firms and to quantify the impact of factors such as race, gender, and age on callback rates.\nWhy this Data: We selected this dataset because it not only addresses pressing issues of discrimination and equity but also includes rich information (e.g., date, location, and applicant demographics) that supports extensive exploratory and inferential analysis.\n\n\n\nCensus Population, Demographic Data\n\nSource: The census data was obtained from the United States Census Bureau.\nLink to census data and Link to census data\nPurpose: The census data provides demographic information about the population in each state, which is essential for understanding the the size of the population in each state and how it relates to labor market dynamics.\nWhy this Data: The census data is crucial for contextualizing the experimental data, allowing us to compare relative submission versus the number of applications in the study. This enables us to assess the representativeness of the sample and to explore potential biases in the data."
  },
  {
    "objectID": "data.html#data-files-and-variables",
    "href": "data.html#data-files-and-variables",
    "title": "Data",
    "section": "Data Files and Variables",
    "text": "Data Files and Variables\n\nExperimental Dataset\nThe project focuses on a single dataset that was processed and saved as an RDS file (cleaned_data.rds). Some Key variables include:\n\nage_at_sub: Age of the applicant at the time of submission.\nmonth and year: Date components of when applications were submitted.\nstate: Geographic information about the submission.\nrace: Race of the applicant.\ncb: Binary indicator for whether an applicant received a callback (1 = Yes, 0 = No).\nAdditional variables (e.g., gender, education, etc.) are available and grouped.\n\nFor a detailed account of variable definitions and transformations, please refer to our cleaning script.\n\n\nCensus Dataset\nTo see how we combine the census data with the application data, please refer to our census cleaning script"
  },
  {
    "objectID": "data.html#data-cleaning-process",
    "href": "data.html#data-cleaning-process",
    "title": "Data",
    "section": "Data Cleaning Process",
    "text": "Data Cleaning Process\n\nThe raw data was imported, cleaned, and transformed using R. The cleaning process involved:\n\nRenaming variables and recoding factors for clarity.\nRemoving duplicate and inconsistent entries.\nAggregating multiple data files (if applicable) to produce the final cleaned dataset.\nSaving the cleaned dataset as an RDS file for efficient reloading in analyses.\n\nFor census data and aggregation, we follow the process below:\n\nVariables were renamed and factors recoded for clarity.\nDuplicate and inconsistent entries were eliminated.\nIndividual application records were aggregated to the state level.\nThe state-level application data was merged with census population counts to calculate the proportion of applications relative to state populations."
  },
  {
    "objectID": "data.html#diagnostic-plots",
    "href": "data.html#diagnostic-plots",
    "title": "Data",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\nBelow are some initial diagnostic plots generated from the cleaned dataset.\n\nImporting the Cleaned Data\n\nrm(list = ls())\n# hide warnings\noptions(warn = -1)\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(stargazer))\nsuppressPackageStartupMessages(library(usmap)) # used for plotting US maps\ndata &lt;- readRDS(\"dataset/cleaned_data.rds\")\nmerged_data &lt;- readRDS(\"dataset/merged_data_state_by_year.rds\")\n\n\n\nCheck for outliers in Age at Submission\n\nboxplot(data$age_at_sub, main = \"Age at Submission\")\n\n\n\n\n\n\n\n\n\n\nCheck distribution of submissions throughout years\n\nggplot(data, aes(x = month)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +  \n  scale_x_continuous(\n    breaks = 1:12,\n    labels = month.abb\n  ) +\n  labs(\n    title = \"Distribution of Months\",\n    x = \"Month\",\n    y = \"Count\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45)) +\n  facet_grid(~year)\n\n\n\n\n\n\n\n\n\n\nDistribution of Submissions by State\n\ndata %&gt;%\n  count(state) %&gt;%\n  arrange(desc(n)) %&gt;% \n  ggplot(aes(y = reorder(state, n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Distribution of States (Ordered by Count)\",\n    y = \"State\",\n    x = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\nGeographic Distribution of Submissions Versus Population By State\n\ncross_sectional &lt;- merged_data %&gt;%\n  group_by(state) %&gt;%\n  summarise(\n    num_sub = sum(num_sub),\n    callback_rate = mean(callback_rate),\n    age = mean(age),\n    name_length = mean(name_length),\n    proportion_black_name = sum(proportion_black_name * num_sub) / num_sub,\n    population = mean(population),\n    proportion_of_applicants = num_sub / population\n  ) %&gt;%\n  ungroup()\n\n\nplot_usmap(data = cross_sectional, values = \"population\") +\n  scale_fill_continuous(low = \"white\", high = \"skyblue\", \n                        name = \"Population Size\", \n                        label = scales::comma) +\n  labs(title = \"Mean Population Size of Each State 2019-2021\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nplot_usmap(data = cross_sectional, values = \"num_sub\") +\n  scale_fill_continuous(low = \"white\", high = \"skyblue\", \n                        name = \"Total Number of Applicantions\", \n                        label = scales::comma) +\n  labs(title = \"Total Number of Applicantions Within State 2019-2021\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nThis comes from the original file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset-ignore folder which you will have to create manually. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Stage and commit the files just like you would any other file.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour clean_data.R file in the scripts folder is the file where you will import the raw data that you download, clean it, and write .rds file(s) (using write_rds) that you’ll load in your analysis page. If desirable, you can have multiple scripts that produce different derived data sets, just make sure to link to them on this page.\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\). Instead, use the here function from the here package to avoid path problems.\n\n\nClean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which will usually be .rds files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones, possibly grouping together variables that are similar, and summarize the rest.\nUse figures or tables to help explain the data. For example, showing a histogram or bar chart for a particularly important variable can provide a quick overview of the values that variable tends to take.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your clean_data.R file.\nRename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nCode Reference:"
  },
  {
    "objectID": "analysis.html#loading-data",
    "href": "analysis.html#loading-data",
    "title": "Analysis",
    "section": "Loading data",
    "text": "Loading data\n\nrm(list = ls())\n# hide warnings\noptions(warn = -1)\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(stargazer))\nsuppressPackageStartupMessages(library(readxl))\nsuppressPackageStartupMessages(library(fixest))\nsuppressPackageStartupMessages(library(usmap))\nsuppressPackageStartupMessages(library(lfe))    \nsuppressPackageStartupMessages(library(lmtest))\nsuppressPackageStartupMessages(library(sandwich)) \nsuppressPackageStartupMessages(library(multiwayvcov))\nsuppressPackageStartupMessages(library(here))\nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(htmltools))\nsuppressPackageStartupMessages(library(broom))\nsuppressPackageStartupMessages(library(stringr))\n\ndata &lt;- readRDS(here::here(\"dataset/cleaned_data.rds\"))"
  },
  {
    "objectID": "analysis.html#motivation",
    "href": "analysis.html#motivation",
    "title": "Analysis",
    "section": "Motivation",
    "text": "Motivation\nDespite decades of regulatory and legal efforts to eliminate hiring discrimination, numerous field experiments continue to document persistent gaps in employer callback rates by race. In the study by Rose et al. (2022), the large scale fake resume experiment among the Fortune 500 firms in the US enables us to investigate the extent of taste based discrimination in hiring practices.\nIn a nutshell, the study sends fake resumes with randomly assigned characteristics to entry level positions among large U.S employers. This is an example of how a sample of resumes looks like:\n\n\nNote: sourced from Figure A1: Examples of applicant resumes in the Rose et al. (2022) paper."
  },
  {
    "objectID": "analysis.html#research-questions",
    "href": "analysis.html#research-questions",
    "title": "Analysis",
    "section": "Research Questions",
    "text": "Research Questions\nThere is a famous saying:\n\nCorrelation does not imply causation.\n\nThis is especially true when it comes to the dialogue of racial disparity from the data-driven point of view. In the previous correlation based analysis, the discrimination gap may be mis-specified due to omitted variable bias (OVB). For instance, instead of taste-based discrimination, the labor market outcome inequality may be driven by different level of educational attainment, work experience, or other factors that are not controlled for.\nThis analysis leverages the powerful experimental data and codes from the Rose et al. (2022) paper to estimate the causal impact of being Black on the probability of receiving a callback.\nThe analysis is based on the following research questions:\n\nDoes tasted based discrimination cause differential labor market outcome in the U.S?\n\n\nIf so, how large is the magnitude?\n\n\nIs there evidence of intersectional heterogeneity—that is, does the magnitude of the Black callback penalty vary by another trait?\n\n\n\nIf so, how large is the magnitude?\n\n\nDo firm‑ or month‑level shocks attenuate or amplify measured discrimination?"
  },
  {
    "objectID": "analysis.html#summary-statistics",
    "href": "analysis.html#summary-statistics",
    "title": "Analysis",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\ndata2 &lt;- data %&gt;%\n  filter(balanced &gt;= 0) %&gt;% \n  mutate(\n    balanced_group = if_else(balanced &gt;= 1, \"Balanced\", \"All Firms\"),\n    race = if_else(black == 1, \"Black\", \"White\"),\n    subgroup = paste(balanced_group, race, sep = \" - \")\n  )\n\ntbl &lt;- data2 %&gt;%\n  tbl_summary(\n    by = subgroup,                  \n    include = c(cb, call_cb, email_cb, text_cb),  # select variables to summarize\n    missing = \"ifany\",                  # do not list missing data separately\n    statistic = list(all_continuous() ~ \"{n} / {mean} / {sd} / {median}/ {p25}/ {p75}\"),\n  ) %&gt;% \n  modify_header(\n    label = \"**Variable**\",\n    stat_by = \"**N / Mean / SD**\"\n  ) \n\ntbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nAll Firms - Black\nN = 9,1411\nAll Firms - White\nN = 9,1341\nBalanced - Black\nN = 32,6651\nBalanced - White\nN = 32,7031\n\n\n\n\nAny contact\n1,972 (22%)\n2,103 (23%)\n7,647 (23%)\n8,380 (26%)\n\n\nAny voicemail contact\n1,246 (14%)\n1,383 (15%)\n5,410 (17%)\n6,052 (19%)\n\n\nAny email contact\n256 (2.8%)\n263 (2.9%)\n1,357 (4.2%)\n1,414 (4.3%)\n\n\nAny text message contact\n470 (5.1%)\n457 (5.0%)\n880 (2.7%)\n914 (2.8%)\n\n\n\n1 n (%)"
  },
  {
    "objectID": "analysis.html#descriptive-evidence-by-applicants-first-name",
    "href": "analysis.html#descriptive-evidence-by-applicants-first-name",
    "title": "Analysis",
    "section": "Descriptive Evidence by Applicants First Name",
    "text": "Descriptive Evidence by Applicants First Name\n\ndf &lt;- data\nols &lt;- felm(cb ~ black + white - 1|0|0|job_id, data = df)\nols2 &lt;- felm(cb ~ factor(firstname) - 1|0|0|job_id, data = df)\ncoef &lt;- as.data.frame(summary(ols2)$coefficients)\ncoef &lt;- coef %&gt;% rownames_to_column(var='coef') %&gt;%\n  extract(coef, \"firstname\", \"\\\\(firstname\\\\)([A-Za-z]+)\", remove=TRUE)\ncoef &lt;- df %&gt;% select(race,gender,firstname) %&gt;% distinct %&gt;%\n  right_join(coef, by='firstname') %&gt;% arrange(race,gender,firstname) %&gt;%\n  mutate(order=row_number())\ncoef[,1:3]&lt;- lapply(coef[, 1:3], as.factor)\n\ncoef &lt;- coef %&gt;% mutate(\"Race and gender\"=case_when(\n      race == \"Black\" & gender == \"Female\" ~ \"Black, female\",\n      race == \"Black\" & gender == \"Male\" ~ \"Black, male\",\n      race == \"White\" & gender == \"Female\" ~ \"White, female\",\n      race == \"White\" & gender == \"Male\" ~ \"White, male\"))\n\nplot_A3 &lt;- ggplot(data = coef, \n       aes(x = reorder(factor(firstname), order), y = Estimate, fill = `Race and gender`)) +\n  geom_bar(stat = 'identity', alpha = .3) + \n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + \n  labs(\n    x = \"Applicant first name\",\n    y = \"Mean 30-day callback rate\",\n    fill = \"Race and gender\"\n  ) +\n  geom_segment(aes(x = 1, y = coef(ols)['black'], xend = 38, yend = coef(ols)['black']),\n               show.legend = FALSE, color = \"#F8766D\") +\n  geom_segment(aes(x = 39, y = coef(ols)['white'], xend = 76, yend = coef(ols)['white']),\n               show.legend = FALSE, color = \"#00BFCF\")\n\nprint(plot_A3, height = 5, width = 8)\n\n\n\n\n\n\n\n\nThis figure shows mean contact rates by applicant first name, organized by race and gender group. The horizontal bars show race group mean contact rates."
  },
  {
    "objectID": "analysis.html#causal-effect-of-taste-based-discrimination-on-contact-rates",
    "href": "analysis.html#causal-effect-of-taste-based-discrimination-on-contact-rates",
    "title": "Analysis",
    "section": "Causal Effect of Taste Based Discrimination on Contact Rates",
    "text": "Causal Effect of Taste Based Discrimination on Contact Rates\nDue to the nature of experimental data, we are able to estimate the causal effect of large employers’ taste based discrimination on applicant labor market outcomes.\n\nModel Specification\nThe code estimates the causal effect of being Black on the probability of receiving a callback, controlling for a vector of other resume characteristics. Formally, two specifications:\n\nLinear probability model (OLS):\n\n\nwith controls for all sample\nwith controls for balanced sample\ntwo-way fixed effects for all sample\ntwo-way fixed effects for balanced sample\n\n\nLogistic regression\n\n\nwith controls for all sample\nwith controls for balanced sample\n\n\ndata &lt;- data %&gt;%\n  mutate(\n    submitted_date = as.Date(submitted_date) \n  ) %&gt;%\n  mutate(\n    first_day_month = floor_date(submitted_date, unit = \"month\")\n  ) %&gt;%\n  mutate(\n    wday_first = wday(first_day_month, week_start = 1),\n    Monday_of_month = first_day_month + days((8 - wday_first) %% 7)\n  ) %&gt;%\n  arrange(submitted_date) %&gt;%\n  mutate(\n    num_month = dense_rank(Monday_of_month)\n  ) %&gt;%\n  select(-first_day_month, -wday_first)\n\n\n\n\ndata_all &lt;- data %&gt;% filter(balanced &gt;= 0) %&gt;%\n  mutate(wave = factor(wave),\n         region4 = factor(region4))\ndata_bal &lt;- data %&gt;% filter(balanced &gt;= 1) %&gt;%\n  mutate(wave = factor(wave),\n         region4 = factor(region4))\n\n\nreg_formula &lt;- as.formula(\"cb ~ black + female + over40 + political_club + academic_club + \n                           lgbtq_club + same_gender_pronouns + gender_neutral_pronouns + \n                           associates + wave + region4\")\n\n# Linear Probability Model (LPM)\nfit_lpm_all &lt;- lm(reg_formula, data = data_all)\n# Clustered standard errors by job_id\ncl_vcov_all &lt;- cluster.vcov(fit_lpm_all, data_all$job_id)\nlpm_all_coefs &lt;- coeftest(fit_lpm_all, vcov = cl_vcov_all)\n\n# (Two way LPM) for all sample\nfit_lpm_all_lm_fe &lt;- lm(\n  cb ~ black + factor(firm_id) + factor(num_month)\n  , data = data_all\n)\ncl_vcov_fe_all &lt;- cluster.vcov(fit_lpm_all_lm_fe, data_all$job_id)\nlpm_all_fe_coefs &lt;- coeftest(fit_lpm_all_lm_fe, vcov = cl_vcov_fe_all)\n\n# Logit Model for all firms\nfit_logit_all &lt;- glm(reg_formula, data = data_all, family = binomial(link = \"logit\"))\ncl_vcov_logit_all &lt;- cluster.vcov(fit_logit_all, data_all$job_id)\nlogit_all_coefs &lt;- coeftest(fit_logit_all, vcov = cl_vcov_logit_all)\n\n#print(lpm_all_coefs)\n#print(logit_all_coefs)\n\n\n# (LPM) for balanced sample\nfit_lpm_bal &lt;- lm(reg_formula, data = data_bal)\ncl_vcov_bal &lt;- cluster.vcov(fit_lpm_bal, data_bal$job_id)\nlpm_bal_coefs &lt;- coeftest(fit_lpm_bal, vcov = cl_vcov_bal)\n\n# (Two way LPM) for balanced sample\nfit_lpm_bal_lm_fe &lt;- lm(\n  cb ~ black + factor(firm_id) + factor(num_month)\n  , data = data_bal\n)\ncl_vcov_fe_bal &lt;- cluster.vcov(fit_lpm_bal_lm_fe, data_bal$job_id)\nlpm_bal_fe_coefs &lt;- coeftest(fit_lpm_bal_lm_fe, vcov = cl_vcov_fe_bal)\n\n\n\n# Logit Model for balanced sample\nfit_logit_bal &lt;- glm(reg_formula, data = data_bal, family = binomial(link = \"logit\"))\ncl_vcov_logit_bal &lt;- cluster.vcov(fit_logit_bal, data_bal$job_id)\nlogit_bal_coefs &lt;- coeftest(fit_logit_bal, vcov = cl_vcov_logit_bal)\n\n#print(lpm_bal_coefs)\n#print(logit_bal_coefs)\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\nCall Back Prob.\n\n\n\nOLS\nlogistic\nOLS\nlogistic\n\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n\n\n\n\n\nBlack\n-0.021***\n-0.020***\n-0.115***\n-0.022***\n-0.022***\n-0.123***\n\n\n\n(0.003)\n(0.003)\n(0.016)\n(0.003)\n(0.003)\n(0.018)\n\n\n\n\n\n\n\n\n\n\n\nFemale\n0.0002\n\n0.001\n-0.0002\n\n-0.002\n\n\n\n(0.003)\n\n(0.016)\n(0.003)\n\n(0.018)\n\n\n\n\n\n\n\n\n\n\n\nOver 40\n-0.006**\n\n-0.033**\n-0.005\n\n-0.027\n\n\n\n(0.003)\n\n(0.016)\n(0.003)\n\n(0.018)\n\n\n\n\n\n\n\n\n\n\n\nPolitical club\n-0.002\n\n-0.010\n-0.003\n\n-0.017\n\n\n\n(0.007)\n\n(0.041)\n(0.008)\n\n(0.046)\n\n\n\n\n\n\n\n\n\n\n\nAcademic club\n0.010\n\n0.052\n0.006\n\n0.028\n\n\n\n(0.007)\n\n(0.041)\n(0.008)\n\n(0.046)\n\n\n\n\n\n\n\n\n\n\n\nLGBTQ club\n-0.005\n\n-0.029\n-0.00004\n\n-0.001\n\n\n\n(0.005)\n\n(0.030)\n(0.006)\n\n(0.034)\n\n\n\n\n\n\n\n\n\n\n\nSame‑gender pronouns\n-0.014*\n\n-0.077*\n-0.013\n\n-0.068\n\n\n\n(0.007)\n\n(0.041)\n(0.008)\n\n(0.046)\n\n\n\n\n\n\n\n\n\n\n\nGender‑neutral pronouns\n-0.010\n\n-0.057\n-0.017**\n\n-0.095**\n\n\n\n(0.007)\n\n(0.041)\n(0.009)\n\n(0.048)\n\n\n\n\n\n\n\n\n\n\n\nAssociate degree\n0.001\n\n0.007\n0.003\n\n0.014\n\n\n\n(0.003)\n\n(0.016)\n(0.003)\n\n(0.018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample\nAll Firms\nAll Firms\nAll Firms\nBalanced\nBalanced\nBalanced\n\n\nFirm FE\nNo\nYes\nNo\nNo\nYes\nNo\n\n\nMonth FE\nNo\nYes\nNo\nNo\nYes\nNo\n\n\nObservations\n83,643\n83,643\n83,643\n65,368\n65,368\n65,368\n\n\nR2\n0.024\n0.151\n\n0.024\n0.130\n\n\n\nAdjusted R2\n0.024\n0.150\n\n0.024\n0.129\n\n\n\nF Statistic\n126.977*** (df = 16; 83626)\n120.790*** (df = 123; 83519)\n\n101.025*** (df = 16; 65351)\n112.290*** (df = 87; 65280)\n\n\n\n\n\n\nNote:\n*p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\nClustered SEs by job\n\n\n\n\n\nVisualization of OLS Estimates\n\nterm_labels &lt;- c(\n  black                   = \"Black\",\n  female                  = \"Female\",\n  over40                  = \"Over 40\",\n  gender_neutral_pronouns = \"Gender Neutral pronouns\",\n  associates              = \"Associate degree\",\n  lgbtq_club              = \"LGBTQ club\",\n  political_club          = \"Political club\",\n  academic_club           = \"Academic club\"\n)\n\nmodels_lpm &lt;- list(\n  All      = fit_lpm_all,\n  Balanced = fit_lpm_bal\n)\n\ncoef_df_lpm &lt;- bind_rows(\n  lapply(names(models_lpm), function(sample) {\n    broom::tidy(models_lpm[[sample]]) %&gt;%\n      filter(term %in% names(term_labels)) %&gt;%\n      mutate(\n        lower_ci = estimate - 1.96 * std.error,\n        upper_ci = estimate + 1.96 * std.error,\n        sample   = sample\n      ) %&gt;%\n      select(term, estimate, lower_ci, upper_ci, sample)\n  })\n)\n\ncoef_df_lpm &lt;- coef_df_lpm %&gt;%\n  mutate(\n    term = factor(term,\n                  levels = names(term_labels),\n                  labels = term_labels)\n  )\n\nggplot(coef_df_lpm, aes(x = term, y = estimate, color = sample, shape = sample)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray60\") +\n  geom_point(position = position_dodge(width = 0.7), size = 3) +\n  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci),\n                position = position_dodge(width = 0.7),\n                width = 0.2) +\n  labs(\n    title = \"LPM (OLS) Coefficients for Key Resume Traits (95% CI)\",\n    x        = NULL,\n    y        = \"Coefficient Estimate\",\n    color    = \"Sample\",\n    shape    = \"Sample\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\"\n  )"
  },
  {
    "objectID": "analysis.html#heterogeneity-analysis",
    "href": "analysis.html#heterogeneity-analysis",
    "title": "Analysis",
    "section": "Heterogeneity Analysis",
    "text": "Heterogeneity Analysis\nIn this section, we use interaction terms to explore how the effect of being Black on labor market outcomes varies across different characteristics of the resumes.\n\nvars &lt;- c(\n  \"female\",\n  \"over40\",\n  \"political_club\",\n  \"academic_club\",\n  \"lgbtq_club\",\n  \"same_gender_pronouns\",\n  \"gender_neutral_pronouns\",\n  \"associates\"\n)\n\n\nfull_formula &lt;- as.formula(\n  paste0(\n    \"cb ~ \",\n    paste0(\"black:\", vars, collapse = \" + \"),\n    \" + wave + region4\"\n  )\n)\n\nget_coef_df &lt;- function(data, sample_name) {\n  fit  &lt;- lm(full_formula, data = data)\n  vcov &lt;- cluster.vcov(fit, data$job_id)\n  broom::tidy(fit, vcov = vcov, conf.int = TRUE) %&gt;%\n    filter(str_detect(term, \"^black:\")) %&gt;%\n    mutate(\n      sample = sample_name,\n      pretty_term = recode(\n        term,\n        \"black:female\"                  = \"Black × Female\",\n        \"black:over40\"                  = \"Black × Over 40\",\n        \"black:political_club\"          = \"Black × Political club\",\n        \"black:academic_club\"           = \"Black × Academic club\",\n        \"black:lgbtq_club\"              = \"Black × LGBTQ club\",\n        \"black:same_gender_pronouns\"    = \"Black × Same Gender Pronouns\",\n        \"black:gender_neutral_pronouns\" = \"Black × Gender Neutral Pronouns\",\n        \"black:associates\"              = \"Black × Associate degree\"\n      )\n    )\n}\n\n\ncoef_all  &lt;- get_coef_df(data_all,  \"All\")\ncoef_bal  &lt;- get_coef_df(data_bal,  \"Balanced\")\n\n\ncoef_both &lt;- bind_rows(coef_all, coef_bal) %&gt;%\n  mutate(\n    pretty_term = factor(\n      pretty_term,\n      levels = c(\n        \"Black × Female\",\n        \"Black × Over 40\",\n        \"Black × Political club\",\n        \"Black × Academic club\",\n        \"Black × LGBTQ club\",\n        \"Black × Same Gender Pronouns\",\n        \"Black × Gender Neutral Pronouns\",\n        \"Black × Associate degree\"\n      )\n    )\n  )\n\n\nggplot(coef_both, aes(x = pretty_term, y = estimate, color = sample, shape = sample)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray60\") +\n  geom_point(position = position_dodge(width = 0.7), size = 3) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high),\n                position = position_dodge(width = 0.7),\n                width = 0.2) +\n  labs(\n    title    = \"LPM (OLS) Heterogenous Treatment Effect Estimates (95% CI)\",\n    x        = NULL,\n    y        = \"Estimated Interaction\",\n    color    = \"Sample\",\n    shape    = \"Sample\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    axis.text.x   = element_text(angle = 45, hjust = 1),\n    plot.subtitle = element_text(face = \"italic\", size = 11),\n    legend.position = \"top\"\n  )"
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show long quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data.\n\n\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 5, 2024 at 11:59pm.\nThis comes from the index.qmd file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 7: Interactive Analysis\n\n\n\n\n\nDescription of datasets found. \n\n\n\n\n\nApr 22, 2025\n\n\nTEAMATE\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 6: Wrapping Up the Analysis\n\n\n\n\n\nDescription of datasets found. \n\n\n\n\n\nApr 16, 2025\n\n\nTEAMATE\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 5: Geographic Analysis of Experiment and Census Data\n\n\n\n\n\nDescription of datasets found. \n\n\n\n\n\nApr 7, 2025\n\n\nTEAMATE\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 4: Extended Statistical Modeling\n\n\n\n\n\nDescription of datasets found. \n\n\n\n\n\nMar 31, 2025\n\n\nTEAMATE\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 3: Extended Analysis and Equity Considerations\n\n\n\n\n\nDescription of datasets found. \n\n\n\n\n\nMar 24, 2025\n\n\nTEAMATE\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 2: Data Loading, Cleaning, and Diagnostic Plots\n\n\n\n\n\nDescription of datasets found. \n\n\n\n\n\nMar 17, 2025\n\n\nTEAMATE\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 1: Data Progress\n\n\n\n\n\nDescription of datasets found. \n\n\n\n\n\nFeb 24, 2025\n\n\nTEAMATE\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Tips\n\n\n\n\n\nSome small but important tips to follow. \n\n\n\n\n\nOct 4, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-03-24-blog-3/blog-3.html",
    "href": "posts/2025-03-24-blog-3/blog-3.html",
    "title": "Blog 3: Extended Analysis and Equity Considerations",
    "section": "",
    "text": "rm(list = ls())\n# hide warnings\noptions(warn = -1)\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(stargazer))\nsuppressPackageStartupMessages(library(readxl))\n\ndata &lt;- readRDS(\"dataset/cleaned_data.rds\")\ncensus_data &lt;- read_excel('dataset/NST-EST2024-POP.xlsx')\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`"
  },
  {
    "objectID": "posts/2025-03-24-blog-3/blog-3.html#data-background-and-context",
    "href": "posts/2025-03-24-blog-3/blog-3.html#data-background-and-context",
    "title": "Blog 3: Extended Analysis and Equity Considerations",
    "section": "Data Background and Context",
    "text": "Data Background and Context\nThe dataset originates from the landmark study “Systemic Discrimination Among Large U.S. Employers” (Kline, Rose, and Walters, 2022).\n\nResearch Questions: The study explores whether discrimination is endemic to particular firms, investigates firm-level heterogeneity in callback rates, and considers the potential impact of industry, geographic location, and other structural factors.\n\nThe census data was retrieved from the United States Census Bureau at this link: https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html"
  },
  {
    "objectID": "posts/2025-03-24-blog-3/blog-3.html#extended-exploratory-analysis-and-equity-considerations",
    "href": "posts/2025-03-24-blog-3/blog-3.html#extended-exploratory-analysis-and-equity-considerations",
    "title": "Blog 3: Extended Analysis and Equity Considerations",
    "section": "Extended Exploratory Analysis and Equity Considerations",
    "text": "Extended Exploratory Analysis and Equity Considerations\nIn this update, I extend my analysis to delve deeper into the structure of the data and highlight important equity issues. By examining the distribution of submissions by state and comparing callback rates by race, the analysis aims to uncover potential systemic biases and inform further steps."
  },
  {
    "objectID": "posts/2025-03-24-blog-3/blog-3.html#check-distribution-of-submissions-by-state",
    "href": "posts/2025-03-24-blog-3/blog-3.html#check-distribution-of-submissions-by-state",
    "title": "Blog 3: Extended Analysis and Equity Considerations",
    "section": "Check distribution of submissions by state",
    "text": "Check distribution of submissions by state\nThe following code creates a bar plot that orders states by the count of submissions. This visualization helps identify geographic patterns in the data, which may be related to regional hiring practices or other local factors.\n\ndata %&gt;%\n  count(state) %&gt;%\n  arrange(desc(n)) %&gt;% \n  ggplot(aes(y = reorder(state, n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Distribution of States (Ordered by Count)\",\n    y = \"State\",\n    x = \"Count\"\n  )"
  },
  {
    "objectID": "posts/2025-03-24-blog-3/blog-3.html#check-distribution-of-population-by-state",
    "href": "posts/2025-03-24-blog-3/blog-3.html#check-distribution-of-population-by-state",
    "title": "Blog 3: Extended Analysis and Equity Considerations",
    "section": "Check distribution of population by state",
    "text": "Check distribution of population by state\nThe following code organizes data from the U.S. Census Bureau and creates a bar plot that orders states by their population in 2024. This visualization highlights the distribution of population density across states, providing a clear comparison to the research data. By aligning the population data with the research findings, the plot demonstrates a correlation between states with higher populations and the number of samples collected in those states.\n\ncolnames(census_data)[1] &lt;- \"states\"\n\ncensus_data &lt;- census_data %&gt;% \n  select(-2) %&gt;% \n  slice(-(1:3))\n\ncolnames(census_data)[2:6] &lt;- c(\"2020\", \"2021\", \"2022\", \"2023\", \"2024\")\n\ncensus_data &lt;- census_data %&gt;% filter(grepl(\"^\\\\.\", states))\n\ncensus_data &lt;- census_data %&gt;% \n  filter(states != \".Puerto Rico\") %&gt;%\n  mutate(states = gsub(\"^\\\\.\", \"\", states)) \n\ncensus_data %&gt;%\n  ggplot(aes(y = reorder(states, `2024`), x = `2024`)) +  # Use backticks for column names\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Population Distribution by State (2024)\",\n    y = \"State\",\n    x = \"Population\"\n  )"
  },
  {
    "objectID": "posts/2024-10-04-general-tips/general-tips.html",
    "href": "posts/2024-10-04-general-tips/general-tips.html",
    "title": "General Tips",
    "section": "",
    "text": "Use the tidyverse!\nYou don’t have to tell me what kind of chart something is. For example, the below is not a useful start to a sentence.\n\n\nThe graph presents a horizontal bar chart …\n\n\nEach page should be largely standalone.\nSometimes small tables or even inline numbers are better than a figure.\nRedundant colors (e.g. bar charts where each bar is a different color that doesn’t signify anything) often don’t help.\nProvide some details on how much data was removed in your cleaning process.\nUse the tidyverse!\nImagine I’m an impatient boss. Show me only what is important and relevant.\nCleaning must be entirely in R\nDon’t say things like, well if only everyone did like so and so than everything would be better. There are many things hiding behind the data that would go to explain things. This is an example of a bad conclusion.\n\n\nThe world could benefit form modeling its education systems after Europe’s.\n\nIt is fine to talk about how the European system is better according to certain metrics, but don’t assume that can easily translate to other regions.\n\nDon’t talk about your “journey”. The blog posts tell the story of your journey. The main pages should focus on the data and your findings.\n\n\nUse the tidyverse!\nNo but seriously, when asking ChatGPT to do your project for you, make sure to tell it to use the tidyverse, not base R."
  },
  {
    "objectID": "posts/2025-03-17-blog-2/blog-2.html",
    "href": "posts/2025-03-17-blog-2/blog-2.html",
    "title": "Blog 2: Data Loading, Cleaning, and Diagnostic Plots",
    "section": "",
    "text": "This project uses data from the study “Systemic Discrimination Among Large U.S. Employers” by Patrick Kline, Evan K. Rose, and Christopher Walters (2022). The study examines discrimination in hiring practices through a large-scale correspondence experiment. Key aspects of the study include:\n\nObjective: To detect whether disparate treatment in hiring—particularly based on race, gender, and age—is concentrated within specific companies.\nDesign: A targeted randomized control trial where fictitious applications (varying by race, gender, and other resume characteristics) were submitted to over 100 Fortune 500 firms across multiple waves (including during the COVID pandemic).\nScale: Over 84,000 applications were sent, enabling both firm-level and industry-level analysis of callback rates.\nContext: The data is used to measure systemic discrimination, a term defined by patterns or practices with a broad impact on an industry or geographic area, and to provide actionable intelligence for policy enforcement (e.g., EEOC investigations).\n\nThis background provides important context on both the experimental design and the intended use of the data, highlighting potential challenges such as sample bias and variability across firms.\nIn this post, I describe the initial steps for loading and cleaning the dataset. I begin by reading in the cleaned data from an RDS file. The dataset contains various variables, including age_at_sub, month, year, and state, which I will explore to understand data quality and identify potential issues. Below is some sample R code I developed to generate diagnostic plots. I create a histogram to visualize the distribution of submissions throughout the months, faceted by year.\n\nrm(list = ls())\n# hide warnings\noptions(warn = -1)\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(stargazer))\ndata &lt;- readRDS(\"dataset/cleaned_data.rds\")"
  },
  {
    "objectID": "posts/2025-03-17-blog-2/blog-2.html#data-background",
    "href": "posts/2025-03-17-blog-2/blog-2.html#data-background",
    "title": "Blog 2: Data Loading, Cleaning, and Diagnostic Plots",
    "section": "",
    "text": "This project uses data from the study “Systemic Discrimination Among Large U.S. Employers” by Patrick Kline, Evan K. Rose, and Christopher Walters (2022). The study examines discrimination in hiring practices through a large-scale correspondence experiment. Key aspects of the study include:\n\nObjective: To detect whether disparate treatment in hiring—particularly based on race, gender, and age—is concentrated within specific companies.\nDesign: A targeted randomized control trial where fictitious applications (varying by race, gender, and other resume characteristics) were submitted to over 100 Fortune 500 firms across multiple waves (including during the COVID pandemic).\nScale: Over 84,000 applications were sent, enabling both firm-level and industry-level analysis of callback rates.\nContext: The data is used to measure systemic discrimination, a term defined by patterns or practices with a broad impact on an industry or geographic area, and to provide actionable intelligence for policy enforcement (e.g., EEOC investigations).\n\nThis background provides important context on both the experimental design and the intended use of the data, highlighting potential challenges such as sample bias and variability across firms.\nIn this post, I describe the initial steps for loading and cleaning the dataset. I begin by reading in the cleaned data from an RDS file. The dataset contains various variables, including age_at_sub, month, year, and state, which I will explore to understand data quality and identify potential issues. Below is some sample R code I developed to generate diagnostic plots. I create a histogram to visualize the distribution of submissions throughout the months, faceted by year.\n\nrm(list = ls())\n# hide warnings\noptions(warn = -1)\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(stargazer))\ndata &lt;- readRDS(\"dataset/cleaned_data.rds\")"
  },
  {
    "objectID": "posts/2025-03-17-blog-2/blog-2.html#check-distribution-of-submission-month-throughout-years",
    "href": "posts/2025-03-17-blog-2/blog-2.html#check-distribution-of-submission-month-throughout-years",
    "title": "Blog 2: Data Loading, Cleaning, and Diagnostic Plots",
    "section": "Check distribution of submission month throughout years",
    "text": "Check distribution of submission month throughout years\n\nggplot(data, aes(x = month)) +\n  geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +  \n  scale_x_continuous(\n    breaks = 1:12,\n    labels = month.abb\n  ) +\n  labs(\n    title = \"Distribution of Months\",\n    x = \"Month\",\n    y = \"Count\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45)) +\n  facet_grid(~year)\n\n\n\n\n\n\n\n\nThis plot shows sparse activity in early 2019, with submissions only appearing in a couple of months. In 2020, there is a notable increase, particularly in the latter half of the year. The highest volume of submissions occurs in early 2021, creating a distinct peak in the data. Overall, the distribution suggests that most of the data collection took place from late 2020 into the first months of 2021."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2025-04-22-blog-7/blog-7.html",
    "href": "posts/2025-04-22-blog-7/blog-7.html",
    "title": "Blog 7: Interactive Analysis",
    "section": "",
    "text": "Key objectives:\n\nHigh-level overview: Understand the names of the fake resumes and their callback rates.\nInteractive deep dive: Allow users to explore naming patterns and callback rates by race and gender.\n\n\n# Clear workspace\nrm(list = ls())\n\n\nInteractive Dashboard\nBelow is a self-contained Shiny app chunk that will render directly in your Quarto site using shinylive.\nInstructions:\n- 🔹 Self-contained: All libraries and data loading happen within this chunk.\n- 🔹 Small data: The RDS file is hosted on GitHub Pages for fast loading.\n- 🔹 Embed: Use engine=\"shinylive-r\" and standalone=\"true\" so Quarto knows to spin up a live Shiny session.\n\nlibrary(shiny)\n\nWarning: package 'shiny' was built under R version 4.3.3\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(wordcloud2)\nlibrary(stringdist)\n\n\ndata &lt;- read_rds(\n  \"https://sussmanbu.github.io/ma4615-sp25-final-project-teamate/dataset_for_shiny/cleaned_data.rds\"\n)\n\n#-- UI definition -------------------------------------------------------------\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Name Analysis Dashboard\"),\n  sidebarLayout(\n    sidebarPanel(\n      h4(\"Word Cloud Settings\"),\n      # User selects race and gender to filter names\n      selectInput(\"race_wc\",  \"Select Race:\",   choices = sort(unique(data$race)),   selected = \"White\"),\n      selectInput(\"gender_wc\",\"Select Gender:\", choices = sort(unique(data$gender)), selected = unique(data$gender)[1])\n    ),\n    mainPanel(\n      # Output: word cloud of top 100 first names\n      wordcloud2Output(\"name_wc\", width = \"100%\", height = \"600px\"),\n      br(),\n      # Output: table of top callback-rate names\n      h4(\"Top Names by Callback Rate\"),\n      tableOutput(\"top_callbacks\")\n    )\n  )\n)\n\n#-- Server logic -------------------------------------------------------------\nserver &lt;- function(input, output, session) {\n  # Filter data by selected race & gender\n  filtered_wc &lt;- reactive({\n    req(input$race_wc, input$gender_wc)\n    data %&gt;% filter(race == input$race_wc, gender == input$gender_wc)\n  })\n\n  # Render word cloud: most common first names\n  output$name_wc &lt;- renderWordcloud2({\n    filtered_wc() %&gt;%\n      count(firstname) %&gt;%\n      arrange(desc(n)) %&gt;%\n      head(100) %&gt;%\n      wordcloud2(size = 1)\n  })\n\n  # Render table: names ranked by callback rate\n  output$top_callbacks &lt;- renderTable({\n    filtered_wc() %&gt;%\n      group_by(firstname) %&gt;%\n      summarise(\n        callback_rate = mean(cb, na.rm = TRUE),\n        count = n(),\n        .groups = 'drop'\n      ) %&gt;%\n      arrange(desc(callback_rate), desc(count)) %&gt;%\n      head(10)\n  }, rownames = FALSE)\n}\n\n# Launch Shiny app -----------------------------------------------------------\nshinyApp(ui, server)\n\nShiny applications not supported in static R Markdown documents\n\n\n\nDeployment tips:\n1. Confirm this chunk runs locally as a standard R chunk.\n2. Place the RDS file in your scripts/ folder and push to GitHub.\n3. Update the read_rds() URL to point to your published scripts/ location.\n4. Change the chunk engine to shinylive-r and re-render the site.\n5. Adjust viewerHeight as needed to fit your page layout.\nThis interactive component complements the static analysis by letting readers examine names and callback patterns directly by subgroup."
  },
  {
    "objectID": "posts/2025-04-14-blog-6/blog-6.html",
    "href": "posts/2025-04-14-blog-6/blog-6.html",
    "title": "Blog 6: Wrapping Up the Analysis",
    "section": "",
    "text": "This post replicates the table 1 and 2 in the paper in R.\nNote: Experiment data is from 2019-2021, while census data is from 2019-2024. The following code aggregate application level data to the state level and merge it with the census data to calculate the proportion of applications by state.\nThis exercise will help us understand the background of the data on a state-by-state basis and explore the relationship between callback rates and other variables at the state level.\nrm(list = ls())\n# hide warnings\noptions(warn = -1)\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(stargazer))\nsuppressPackageStartupMessages(library(readxl))\nsuppressPackageStartupMessages(library(fixest))\nsuppressPackageStartupMessages(library(usmap))\nsuppressPackageStartupMessages(library(lfe))    \nsuppressPackageStartupMessages(library(lmtest))\nsuppressPackageStartupMessages(library(sandwich)) \nsuppressPackageStartupMessages(library(multiwayvcov))\n\n\ndata &lt;- readRDS(\"dataset/cleaned_data.rds\")"
  },
  {
    "objectID": "posts/2025-04-14-blog-6/blog-6.html#data-background-and-context",
    "href": "posts/2025-04-14-blog-6/blog-6.html#data-background-and-context",
    "title": "Blog 6: Wrapping Up the Analysis",
    "section": "Data Background and Context",
    "text": "Data Background and Context\nThe dataset originates from the landmark study “Systemic Discrimination Among Large U.S. Employers” (Kline, Rose, and Walters, 2022).\n\nResearch Questions: The study explores whether discrimination is endemic to particular firms, investigates firm-level heterogeneity in callback rates, and considers the potential impact of industry, geographic location, and other structural factors.\n\nThe census data was retrieved from the United States Census Bureau at this link: https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html\n\nfor(k in 1:4) {\n  data[[paste0(\"region4_\", k)]] &lt;- as.integer(data$region4 == k)\n}\n\nfor(k in 1:5) {\n  data[[paste0(\"wave\", k)]] &lt;- as.integer(data$wave == k)\n}"
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2025-04-07-blog-5/blog-5.html",
    "href": "posts/2025-04-07-blog-5/blog-5.html",
    "title": "Blog 5: Geographic Analysis of Experiment and Census Data",
    "section": "",
    "text": "This code continues the same procedure as the previous blog, focusing on the analysis of a combined state-by-year dataset.\nNote: Experiment data is from 2019-2021, while census data is from 2019-2024. The following code aggregate application level data to the state level and merge it with the census data to calculate the proportion of applications by state.\nThis exercise will help us understand the background of the data on a state-by-state basis and explore the relationship between callback rates and other variables at the state level.\nrm(list = ls())\n# hide warnings\noptions(warn = -1)\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(stargazer))\nsuppressPackageStartupMessages(library(readxl))\nsuppressPackageStartupMessages(library(fixest))\nsuppressPackageStartupMessages(library(usmap))\n\n\ndata &lt;- readRDS(\"dataset/cleaned_data.rds\")\ncensus_data &lt;- read_excel('dataset/NST-EST2024-POP.xlsx')\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n\ncensus_data_2 &lt;- read_excel('dataset/nst-est2019-01.xlsx', skip = 3)\n\nNew names:\n• `` -&gt; `...1`\n\n# for census_data_2 only keep year column 13 and row 6 onwards\ncensus_data_2 &lt;- census_data_2 %&gt;%\n  select(1, 13) %&gt;%\n  slice(6:56) \n\n# rename first column to states \ncolnames(census_data_2)[1] &lt;- \"states\"\n\n# delete . in front of state names\ncensus_data_2 &lt;- census_data_2 %&gt;% \n  filter(grepl(\"^\\\\.\", states)) %&gt;%\n  mutate(states = gsub(\"^\\\\.\", \"\", states))"
  },
  {
    "objectID": "posts/2025-04-07-blog-5/blog-5.html#data-background-and-context",
    "href": "posts/2025-04-07-blog-5/blog-5.html#data-background-and-context",
    "title": "Blog 5: Geographic Analysis of Experiment and Census Data",
    "section": "Data Background and Context",
    "text": "Data Background and Context\nThe dataset originates from the landmark study “Systemic Discrimination Among Large U.S. Employers” (Kline, Rose, and Walters, 2022).\n\nResearch Questions: The study explores whether discrimination is endemic to particular firms, investigates firm-level heterogeneity in callback rates, and considers the potential impact of industry, geographic location, and other structural factors.\n\nThe census data was retrieved from the United States Census Bureau at this link: https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html"
  },
  {
    "objectID": "posts/2025-03-31-blog-4/blog-4.html",
    "href": "posts/2025-03-31-blog-4/blog-4.html",
    "title": "Blog 4: Extended Statistical Modeling",
    "section": "",
    "text": "Note: Experiment data is from 2019-2021, while census data is from 2019-2024. The following code aggregate application level data to the state level and merge it with the census data to calculate the proportion of applications by state.\nThis exercise will help us understand the background of the data on a state-by-state basis and explore the relationship between callback rates and other variables at the state level.\nrm(list = ls())\n# hide warnings\noptions(warn = -1)\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(stargazer))\nsuppressPackageStartupMessages(library(readxl))\nsuppressPackageStartupMessages(library(fixest))\n\ndata &lt;- readRDS(\"dataset/cleaned_data.rds\")\ncensus_data &lt;- read_excel('dataset/NST-EST2024-POP.xlsx')\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n\ncensus_data_2 &lt;- read_excel('dataset/nst-est2019-01.xlsx', skip = 3)\n\nNew names:\n• `` -&gt; `...1`\n\n# for census_data_2 only keep year column 13 and row 6 onwards\ncensus_data_2 &lt;- census_data_2 %&gt;%\n  select(1, 13) %&gt;%\n  slice(6:56) \n\n# rename first column to states \ncolnames(census_data_2)[1] &lt;- \"states\"\n\n# delete . in front of state names\ncensus_data_2 &lt;- census_data_2 %&gt;% \n  filter(grepl(\"^\\\\.\", states)) %&gt;%\n  mutate(states = gsub(\"^\\\\.\", \"\", states))"
  },
  {
    "objectID": "posts/2025-03-31-blog-4/blog-4.html#data-background-and-context",
    "href": "posts/2025-03-31-blog-4/blog-4.html#data-background-and-context",
    "title": "Blog 4: Extended Statistical Modeling",
    "section": "Data Background and Context",
    "text": "Data Background and Context\nThe dataset originates from the landmark study “Systemic Discrimination Among Large U.S. Employers” (Kline, Rose, and Walters, 2022).\n\nResearch Questions: The study explores whether discrimination is endemic to particular firms, investigates firm-level heterogeneity in callback rates, and considers the potential impact of industry, geographic location, and other structural factors.\n\nThe census data was retrieved from the United States Census Bureau at this link: https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html"
  },
  {
    "objectID": "posts/2025-02-24-blog-1/blog-1.html",
    "href": "posts/2025-02-24-blog-1/blog-1.html",
    "title": "Blog 1: Data Progress",
    "section": "",
    "text": "Data #1: URL: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HLO4XC\nThe data comes from Kline, Patrick, Evan K. Rose, and Christopher R. Walters, “Systemic Discrimination Among Large U.S. Employers,” The Quarterly Journal of Economics, vol. 137 no. 4, (2022), 1963-2036.https://doi.org/10.1093/qje/qjac024. This dataset, published alongside the article by Kline, Rose, and Walters (2022), examines potential systemic hiring discrimination by large U.S. employers. The data primarily cover hiring outcomes, employer characteristics, and applicant demographic details spanning several years. Data is originally collected by authors. They conducted a randomized control trail (RCT) to identify patterns of potential discrimination, estimating the magnitude of these effects, and understanding whether certain employer or regional characteristics are associated with differential outcomes.\nData #2: URL: https://opportunityinsights.org/data/\nThe dataset described in College-Level Data for 139 Selective American Colleges provides insights into college application and attendance patterns among U.S. students based on parental income. Compiled by linking standardized test-takers (SAT or ACT from 2011, 2013, or 2015) to tax data, this dataset categorizes students into income bins to analyze disparities in college access. It covers 139 selective colleges, including Ivy-Plus institutions, elite private colleges, and flagship public universities. The dataset includes key metrics such as relative application rates, attendance rates, and conditional attendance rates, allowing researchers to examine how income influences higher education opportunities.\nA challenge in working with this dataset is its reliance on estimated values, as noise has been added for privacy reasons, potentially affecting precision. Additionally, while the dataset is rich in application and attendance statistics, it does not include direct measures of student academic performance beyond standardized test scores, limiting deeper analysis of post-admission outcomes.\nData #3 URL: https://www.samhsa.gov/data/data-we-collect/teds-treatment-episode-data-set/datafiles The Treatment Episode Data Set (TEDS), available through the SAMHSA website (TEDS Data Files), provides comprehensive data on individuals entering substance abuse treatment programs across the U.S. It includes over 2 million treatment episodes, with rows representing individual treatment episodes and columns covering demographics, substance use types, treatment modalities, and outcomes. The data is collected through state-administered systems in federally-funded treatment centers, aiming to monitor patterns of substance use, treatment needs, and outcomes. This standardized collection allows for detailed analysis of treatment trends across different regions and populations.\nWhile the dataset is available in multiple formats such as SAS, SPSS, and ASCII, loading and cleaning the data may require addressing missing values, ensuring proper data type conversion, and standardizing categories. A few key questions that can be explored include how treatment outcomes vary by demographics and how substance abuse trends differ across states and over time. Challenges may include the large size of the dataset, handling missing data, and ensuring confidentiality with sensitive information."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMATE.\nThe members of this team are below."
  },
  {
    "objectID": "about.html#acknowledgements",
    "href": "about.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis is a course project for MA[46]15 Data Science with R at the Boston University. We thank Professor Daniel Sussman and TF Aislinn Sullivan for their guidance and support throughout the course. We also thank our classmates for their feedback. The content and assessments for this course have been modified as part of Boston University’s Designing Antiracist Curricula Fellowship.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.qmd.\nIn today’s competitive job market, a candidate’s name can be an unexpected gatekeeper. Resumes bearing names commonly associated with different racial or gender groups may receive different callback rates—even when qualifications are identical. That’s the hidden bias we set out to uncover.\n &gt; Note: Sacred Journey by Michael Reeder\nOur thesis:\nTaste Based Discrimination in initial resume screening manifests in callback rates that vary systematically by race and gender of first names. By analyzing hundreds of thousands of fictitious job applications in a randomized experiment, we can both quantify these disparities and let readers explore them interactively.\n\n\n\nFairness in hiring: Early-stage screening should focus on qualifications, not perceived identity.\n\nData-driven insight: Large-scale resume studies provide rigorous evidence, moving the conversation beyond anecdotes.\n\nActionable takeaways: Organizations can monitor name-based disparities and design blind screening to reduce bias.\n\n\n\n\n\nDescriptive analysis: Static charts show the overall spread of callback rates by race and gender groups.\n\nInteractive deep dive: This page embeds a small Shiny dashboard, letting you explore the most common first names by subgroup and see which names yield higher callback rates.\n\n\n\n\n\nThe interactive below lets you select any racial group and gender to view:\n\nA word cloud of the top first names in that subgroup.\n\nA table of the top names ranked by callback rate.\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| eval: true\n#| standalone: true\n#| viewerHeight: 640\n# ```{shinylive-r} when ready to publish\nlibrary(shiny)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(wordcloud2)\nlibrary(stringdist)\n\noptions(\"readr.edition\" = 1) # keep this to ensure you can download the data\ndata &lt;- read_rds(\"https://sussmanbu.github.io/ma4615-sp25-final-project-teamate/dataset_for_shiny/cleaned_data.rds\")\n\n\n# Define UI for app\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Name Analysis Dashboard\"),\n  sidebarLayout(\n    sidebarPanel(\n      h4(\"Word Cloud Settings\"),\n      selectInput(\"race_wc\", \"Select Race:\", choices = sort(unique(data$race)), selected = \"White\"),\n      selectInput(\"gender_wc\", \"Select Gender:\", choices = sort(unique(data$gender)), selected = unique(data$gender)[1])\n    ),\n    mainPanel(\n      wordcloud2Output(\"name_wc\", width = \"100%\", height = \"600px\"),\n      br(),\n      h4(\"Top Names by Callback Rate\"),\n      tableOutput(\"top_callbacks\")\n    )\n  )\n)\n\n# Define server logic required to draw --\nserver &lt;- function(input, output, session) {\n  filtered_wc &lt;- reactive({\n    req(input$race_wc, input$gender_wc)\n    data %&gt;% filter(race == input$race_wc, gender == input$gender_wc)\n  })\n\n  \n  output$name_wc &lt;- renderWordcloud2({\n    df &lt;- filtered_wc() %&gt;%\n      count(firstname) %&gt;%\n      arrange(desc(n)) %&gt;%\n      head(100)\n    wordcloud2(df, size = 1)\n  })\n\n\n  output$top_callbacks &lt;- renderTable({\n    df &lt;- filtered_wc() %&gt;%\n      group_by(firstname) %&gt;%\n      summarise(\n        callback_rate = mean(cb, na.rm = TRUE),\n        count = n()\n      ) %&gt;%\n      arrange(desc(callback_rate), desc(count)) %&gt;%\n      head(10)\n    df\n  }, rownames = FALSE)\n}\n\n# Create Shiny app ----\nshinyApp(ui = ui, server = server)\nTo get the shinylive-r working.\n\nMake sure your shiny app works as a regular r chunk.\nMake sure that the chunk is completely self-contained. Meaning all packages and data are loaded inside that chunk. It can’t rely on any other chunks.\nFor the data that you are using for shiny, copy the rds file or any other files into the scripts folder, and then publish your website.\nWhere you load in your data, change it to use a URL to the data set which will now be on your website. Something like read_rds(“https://sussmanbu.github.io/ma-4615-fa24-final-project-group-a/scripts/dataset_for_shiny.rds”)\nCheck that the chunk still works as a regular r chunk.\nChange it to a shinylive-r chunk.\nCommit and publish your work.\n\nI recommend keeping the data used for the shiny interactive relatively small, though this isn’t completely necessary.\n\n\n\n\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a how a news article or a magazine story might draw you in. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE.\n\n\n\n\n\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\n\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "",
    "text": "Title\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a how a news article or a magazine story might draw you in. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "",
    "text": "Make a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\n\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]