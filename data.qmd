---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
editor: 
  markdown: 
    wrap: sentence
---

![](images/experiment-data.png)

This page details our data sources, the cleaning process, and provides initial diagnostic plots.
For this project, we work with data from the landmark study [*Systemic Discrimination Among Large U.S. Employers*](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HLO4XC) (Kline, Rose, and Walters, 2022).
The data was originally collected through a fake resume experiment to investigate patterns of hiring discrimination in large US employers, making it highly relevant to both current social justice issues and policy enforcement efforts.

## Data Sources and Rationale 
### Experimental, Main Dataset

-   **Source:** The dataset is available on Harvard Dataverse and was published alongside the study *"Systemic Discrimination Among Large U.S. Employers"*.\
    [Link to original data source](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HLO4XC)
-   **Purpose:** The data were collected to examine whether hiring discrimination is endemic to particular firms and to quantify the impact of factors such as race, gender, and age on callback rates.
-   **Why this Data:** We selected this dataset because it not only addresses pressing issues of discrimination and equity but also includes rich information (e.g., date, location, and applicant demographics) that supports extensive exploratory and inferential analysis.


### Census Population, Demographic Data

-   **Source:** The census data was obtained from the United States Census Bureau.\
    [Link to census data](https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html)
    and [Link to census data](https://www.census.gov/data/tables/time-series/demo/popest/2010s-counties-total.html)
    
-   **Purpose:** The census data provides demographic information about the population in each state, which is essential for understanding the the **size** of the population in each state and how it relates to labor market dynamics.

-   **Why this Data:** The census data is crucial for contextualizing the experimental data, allowing us to compare relative submission versus the number of applications in the study. This enables us to assess the representativeness of the sample and to explore potential biases in the data.

## Data Files and Variables

### Experimental Dataset
The project focuses on a single dataset that was processed and saved as an RDS file (`cleaned_data.rds`).
Some Key variables include: 

- **`age_at_sub`**: Age of the applicant at the time of submission.
- **`month` and `year`**: Date components of when applications were submitted.
- **`state`**: Geographic information about the submission.
- **`race`**: Race of the applicant.
- **`cb`**: Binary indicator for whether an applicant received a callback (1 = Yes, 0 = No).
- Additional variables (e.g., gender, education, etc.) are available and grouped.


For a detailed account of variable definitions and transformations, please refer to our [cleaning script](/scripts/load_and_clean_data.R).

### Census Dataset
To see how we combine the census data with the application data, please refer to our [census cleaning script](/scripts/load_and_clean_data_with_census.R) 

## Data Cleaning Process

![](images/data-import-cheatsheet-thumbs.png)

The raw data was imported, cleaned, and transformed using R.
The cleaning process involved: 

- Renaming variables and recoding factors for clarity.
- Removing duplicate and inconsistent entries.
- Aggregating multiple data files (if applicable) to produce the final cleaned dataset.
- Saving the cleaned dataset as an RDS file for efficient reloading in analyses.

For census data and aggregation, we follow the process below:

- Variables were renamed and factors recoded for clarity.
- Duplicate and inconsistent entries were eliminated.
- Individual application records were aggregated to the state level.
- The state-level application data was merged with census population counts to calculate the proportion of applications relative to state populations.

## Diagnostic Plots

Below are some initial diagnostic plots generated from the cleaned dataset.

### Importing the Cleaned Data

```{r, echo=FALSE}
rm(list = ls())
# hide warnings
options(warn = -1)
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(stargazer))
suppressPackageStartupMessages(library(usmap)) # used for plotting US maps
data <- readRDS("dataset/cleaned_data.rds")
merged_data <- readRDS("dataset/merged_data_state_by_year.rds")
```

### Check for outliers in Age at Submission

```{r, echo=FALSE}
boxplot(data$age_at_sub, main = "Age at Submission")
```

### Check distribution of submissions throughout years

```{r, echo=FALSE}
ggplot(data, aes(x = month)) +
  geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black") +  
  scale_x_continuous(
    breaks = 1:12,
    labels = month.abb
  ) +
  labs(
    title = "Distribution of Months",
    x = "Month",
    y = "Count"
  ) +
  theme(axis.text.x = element_text(angle = 45)) +
  facet_grid(~year)
```

### Distribution of Submissions by State

```{r, echo=FALSE}
data %>%
  count(state) %>%
  arrange(desc(n)) %>% 
  ggplot(aes(y = reorder(state, n), x = n)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(
    title = "Distribution of States (Ordered by Count)",
    y = "State",
    x = "Count"
  )
```

### Geographic Distribution of Submissions Versus Population By State
```{r, echo=FALSE}
cross_sectional <- merged_data %>%
  group_by(state) %>%
  summarise(
    num_sub = sum(num_sub),
    callback_rate = mean(callback_rate),
    age = mean(age),
    name_length = mean(name_length),
    proportion_black_name = sum(proportion_black_name * num_sub) / num_sub,
    population = mean(population),
    proportion_of_applicants = num_sub / population
  ) %>%
  ungroup()


plot_usmap(data = cross_sectional, values = "population") +
  scale_fill_continuous(low = "white", high = "skyblue", 
                        name = "Population Size", 
                        label = scales::comma) +
  labs(title = "Mean Population Size of Each State 2019-2021") +
  theme(legend.position = "right")
 
```

```{r, echo=FALSE}
plot_usmap(data = cross_sectional, values = "num_sub") +
  scale_fill_continuous(low = "white", high = "skyblue", 
                        name = "Total Number of Applicantions", 
                        label = scales::comma) +
  labs(title = "Total Number of Applicantions Within State 2019-2021") +
  theme(legend.position = "right")
```
------------------------------------------------------------------------

This comes from the original file `data.qmd`.

Your first steps in this project will be to find data to work on.

I recommend trying to find data that interests you and that you are knowledgeable about.
A bad example would be if you have no interest in video games but your data set is about video games.
I also recommend finding data that is related to current events, social justice, and other areas that have an impact.

Initially, you will study *one dataset* but later you will need to combine that data with another dataset.
For this reason, I recommend finding data that has some date and/or location components.
These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable.
Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components.

## What makes a good data set?

-   Data you are interested in and care about.
-   Data where there are a lot of potential questions that you can explore.
-   A data set that isn't completely cleaned already.
-   Multiple sources for data that you can combine.
-   Some type of time and/or location component.

## Where to keep data?

Below 50mb: In `dataset` folder

Above 50mb: In `dataset-ignore` folder which you will have to create manually.
This folder will be ignored by `git` so you'll have to manually sync these files across your team.

### Sharing your data

For small datasets (\<50mb), you can use the `dataset` folder that is tracked by github.
Stage and commit the files just like you would any other file.

For larger datasets, you'll need to create a new folder in the project root directory named `dataset-ignore`.
This will be ignored by git (based off the `.gitignore` file in the project root directory) which will help you avoid issues with Github's size limits.
Your team will have to manually make sure the data files in `dataset-ignore` are synced across team members.

Your [clean_data.R](/scripts/clean_data.R) file in the `scripts` folder is the file where you will import the raw data that you download, clean it, and write `.rds` file(s) (using `write_rds`) that you'll load in your analysis page.
If desirable, you can have multiple scripts that produce different derived data sets, just make sure to link to them on this page.

You should never use absolute paths (eg. `/Users/danielsussman/path/to/project/` or `C:\MA415\\Final_Project\`).
Instead, use the `here` function from the [`here` package](https://here.r-lib.org/articles/here.html) to avoid path problems.

### Clean data script

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them.
This file might create a derivative data set that you then use for your subsequent analysis.
Note that you don't need to run this script from every post/page.
Instead, you can load in the results of this script, which will usually be `.rds` files.
In your data page you'll describe how these results were created.
If you have a very large data set, you might save smaller data sets that you can use for exploration purposes.

## Rubric: On this page

You will

-   Describe where/how to find data.
    -   You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
    -   Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
-   Describe the different data files used and what each variable means.
    -   If you have many variables then only describe the most relevant ones, possibly grouping together variables that are similar, and summarize the rest.
    -   Use figures or tables to help explain the data. For example, showing a histogram or bar chart for a particularly important variable can provide a quick overview of the values that variable tends to take.
-   Describe any cleaning you had to do for your data.
    -   You *must* include a link to your `clean_data.R` file.
    -   Rename variables and recode factors to make data more clear.
    -   Also, describe any additional R packages you used outside of those covered in class.
    -   Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
    -   Some repetition of what you do in your `clean_data.R` file is fine and encouraged if it helps explain what you did.
-   Organization, clarity, cleanliness of the page
    -   Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
    -   This page should be self-contained.
